## Project Proposal
### Title
Rapid Word Learning Under Uncertainty via Cross-Situational Statistics
### Executive Summary
This research explores the complex challenge of word-to-word pairings in naturalistic learning environments, addressing the indeterminacy problem posited by Quine (1960). While previous studies have primarily focused on constraints within single trials to facilitate word learning, this article proposes a novel approach: leveraging cross-situational learning strategies that compute distributional statistics across words and their referents over multiple trials.
In a series of experiments, adult participants were exposed to trials featuring multiple spoken words and pictures of individual objects, without explicit information on word-referent correspondences. Through varying degrees of reference uncertainty, trial lengths, and trial counts, the results reveal that learners adeptly calculate cross-trial statistics, enabling them to discern word-referent mappings even in highly ambiguous contexts. This ability suggests that learners can store potential word-referent pairings across trials and evaluate statistical evidence to resolve ambiguities.
The findings contribute to a critical gap in our understanding of word learning by demonstrating that human learners can engage in effective cross-situational learning despite the clutter and complexity of real-world environments. By assessing the underlying processes of this statistical learning mechanism, this research paves the way for further exploration into word acquisition beyond traditional fast-mapping methods. The implications extend to theories of language learning and cognitive development, offering new insights into how individuals navigate and interpret linguistic information in diverse contexts.
### Attendees
We need at least 50 attendees for the two experiments.
### Methods
#### Experiment 1
Utilized slides displaying about 50 unique objects (e.g., canister, facial sauna, rasp) paired with corresponding pseudowords generated by a computer program to adhere to phonotactic norms of English.These pseudowords were produced by a synthetic female voice in a monotone format.
Participants were exposed to three distinct learning conditions based on the number of words and referents presented per trial:
2-2 Condition: 2 words and 2 pictures
3-3 Condition: 3 words and 3 pictures
4-4 Condition: 4 words and 4 pictures
Each training trial presented a random pairing of the words with the pictures, without indicating which picture corresponded to which word.
Participants experienced six repetitions of each word-referent pair across trials, allowing for exposure to statistical relationships.
#### Experiment 2
Experiment 2 maintained the 4-4 presentation format from Experiment 1, featuring 4 words and 4 pictures per trial.
The conditions varied in the number of word-referent pairs to be learned:
Condition 1: 9 word-referent pairs with 8 repetitions each
Condition 2: 9 word-referent pairs with 12 repetitions each
Condition 3: 18 word-referent pairs with 6 repetitions each (replicating the 4-4 condition from Experiment 1)
All three conditions used the same set of stimuli as in Experiment 1, but the number of pairs and repetitions differed.
Random selection of co-occurring word-referent pairs during training led to variations in the probabilities of encountering incorrect referents (foils) at test.
Expected the conditions with fewer word-referent pairs (i.e., the 9-word conditions) were anticipated to improve learning performance due to reduced ambiguity and increased frequency of exposure to each word-referent pairing compared to the condition with 18 word-referent pairs.
### Appendices
1. Extend audio & image stimuli
2. add eyetracking


<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>JsPsych Eye-tracking Experiment</title>
    <script src="https://unpkg.com/jspsych@7.2.0/jspsych.js"></script>
    <link rel="stylesheet" href="https://unpkg.com/jspsych@7.2.0/css/jspsych.css">
    <script src="https://webgazer.cs.brown.edu/webgazer.js"></script>
</head>
<body>
    <script>
        // Initialize WebGazer
        window.onload = function() {
            webgazer.setRegression('ridge').begin().showPredictionPoints(true);
            calibrateWebGazer();
        };

        // Calibration function
        function calibrateWebGazer() {
            let calibrationPoints = [[50, 50], [window.innerWidth - 50, 50], [window.innerWidth - 50, window.innerHeight - 50], [50, window.innerHeight - 50]];
            calibrationPoints.forEach(point => {
                let pointElem = document.createElement('div');
                pointElem.style.position = "absolute";
                pointElem.style.left = point[0] + 'px';
                pointElem.style.top = point[1] + 'px';
                pointElem.style.width = "20px";
                pointElem.style.height = "20px";
                pointElem.style.backgroundColor = "red";
                document.body.appendChild(pointElem);

                pointElem.addEventListener('click', () => {
                    webgazer.recordScreenPosition(point[0], point[1]);
                });
            });
        }

        // Define JsPsych timeline
        let experiment_timeline = [];

        let trial = {
            type: 'html-keyboard-response',
            stimulus: '<p>Look at the center of the screen</p>',
            on_load: function() {
                webgazer.setGazeListener(function(data, timestamp) {
                    if (data) {
                        jsPsych.data.write({
                            x: data.x,
                            y: data.y,
                            time: timestamp
                        });
                    }
                });
            },
            on_finish: function() {
                webgazer.clearGazeListener();
            }
        };

        experiment_timeline.push(trial);

        jsPsych.init({
            timeline: experiment_timeline,
            on_finish: function() {
                jsPsych.data.displayData();
            }
        });
    </script>
</body>
</html>



