## Project Proposal
### Title
Rapid Word Learning Under Uncertainty via Cross-Situational Statistics
### Executive Summary
This research explores the complex challenge of word-to-word pairings in naturalistic learning environments, addressing the indeterminacy problem posited by Quine (1960). While previous studies have primarily focused on constraints within single trials to facilitate word learning, this article proposes a novel approach: leveraging cross-situational learning strategies that compute distributional statistics across words and their referents over multiple trials.
In a series of experiments, adult participants were exposed to trials featuring multiple spoken words and pictures of individual objects, without explicit information on word-referent correspondences. Through varying degrees of reference uncertainty, trial lengths, and trial counts, the results reveal that learners adeptly calculate cross-trial statistics, enabling them to discern word-referent mappings even in highly ambiguous contexts. This ability suggests that learners can store potential word-referent pairings across trials and evaluate statistical evidence to resolve ambiguities.
The findings contribute to a critical gap in our understanding of word learning by demonstrating that human learners can engage in effective cross-situational learning despite the clutter and complexity of real-world environments. By assessing the underlying processes of this statistical learning mechanism, this research paves the way for further exploration into word acquisition beyond traditional fast-mapping methods. The implications extend to theories of language learning and cognitive development, offering new insights into how individuals navigate and interpret linguistic information in diverse contexts.
### Attendees
We need at least 50 attendees for the two experiments.
### Justification for Experiment Choice
As AI systems increasingly strive to mimic human cognitive processes, understanding how individuals learn word-referent pairings under uncertainty can provide valuable insights into the development of more sophisticated AI models for natural language processing. This experiment examines the mechanisms by which human learners leverage statistical regularities across multiple trials to derive meaning from ambiguous linguistic inputs, mirroring the way machine learning algorithms extract patterns from data. By investigating how varying conditions of word and referent pairings affect learning outcomes, this research has the potential to inform the design of AI systems that can learn from limited data and ambiguous contexts, thereby enhancing their capacity for language understanding. Ultimately, this experiment not only contributes to theoretical discussions on language learning but also has practical implications for advancing AI applications in language-related tasks.
### Methods
#### Experiment 1
Utilized slides displaying about 50 unique objects (e.g., canister, facial sauna, rasp) paired with corresponding pseudowords generated by a computer program to adhere to phonotactic norms of English.These pseudowords were produced by a synthetic female voice in a monotone format.
Participants were exposed to three distinct learning conditions based on the number of words and referents presented per trial:
**2-2 Condition**: 2 words and 2 pictures
**3-3 Condition**: 3 words and 3 pictures
**4-4 Condition**: 4 words and 4 pictures
Each training trial presented a random pairing of the words with the pictures, without indicating which picture corresponded to which word.
Participants experienced six repetitions of each word-referent pair across trials, allowing for exposure to statistical relationships.
#### Experiment 2
Experiment 2 maintained the 4-4 presentation format from Experiment 1, featuring 4 words and 4 pictures per trial.
The conditions varied in the number of word-referent pairs to be learned:
**Condition 1**: 9 word-referent pairs with 8 repetitions each
**Condition 2**: 9 word-referent pairs with 12 repetitions each
**Condition 3**: 18 word-referent pairs with 6 repetitions each (replicating the 4-4 condition from Experiment 1)
All three conditions used the same set of stimuli as in Experiment 1, but the number of pairs and repetitions differed.
Random selection of co-occurring word-referent pairs during training led to variations in the probabilities of encountering incorrect referents (foils) at test.
Expected the conditions with fewer word-referent pairs (i.e., the 9-word conditions) were anticipated to improve learning performance due to reduced ambiguity and increased frequency of exposure to each word-referent pairing compared to the condition with 18 word-referent pairs.
### Description of Stimuli and Procedures
To conduct these experiments, we will need to create a series of slides featuring approximately 50 unique objects, each paired with a pseudoword that adheres to the phonotactic norms of English. The stimuli will include images of the objects and corresponding audio clips of the pseudowords produced by a synthetic voice. Each participant will be randomly assigned to one of the learning conditions and exposed to the slides in a controlled environment.
The procedures will involve presenting each participant with the learning slides, allowing them to listen to the pseudowords while viewing the corresponding objects. Participants will then complete a test phase where they will be asked to match pseudowords to the correct objects. Throughout this process, data will be collected on the accuracy of their responses to assess their learning performance.
### Challenges
One of the primary challenges will be setting up a user-friendly website to conduct the experiments effectively. This platform must be designed to present the stimuli seamlessly, allowing participants to view images, listen to audio clips, and respond without technical difficulties. Ensuring compatibility across various devices and browsers will be essential to accommodate diverse participant access.
Another challenge involves the effort required to match pseudowords with their corresponding images and audio files accurately. Creating a coherent and engaging user experience will necessitate meticulous attention to detail, including verifying that each audio clip is correctly aligned with its respective image. This process will require extensive testing to ensure that the pairing of words, images, and sounds is intuitive for participants, minimizing confusion during the learning trials.
Additionally, incorporating eye-tracking functions presents its own set of challenges. Implementing eye-tracking technology will require specialized software that can accurately capture and analyze participantsâ€™ visual attention during the trials. This involves not only selecting appropriate eye-tracking hardware but also ensuring that the testing environment is controlled to minimize external distractions. Developing the necessary protocols for integrating eye-tracking data with learning outcomes will be crucial for obtaining insights into how attention dynamics influence word learning.
### Link to the repository and original paper

### Appendices
1. Extend audio & image stimuli
The NOUN database: http://www.sussex.ac.uk/wordlab/noun
3. add eyetracking


<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>JsPsych Eye-tracking Experiment</title>
    <script src="https://unpkg.com/jspsych@7.2.0/jspsych.js"></script>
    <link rel="stylesheet" href="https://unpkg.com/jspsych@7.2.0/css/jspsych.css">
    <script src="https://webgazer.cs.brown.edu/webgazer.js"></script>
</head>
<body>
    <script>
        // Initialize WebGazer
        window.onload = function() {
            webgazer.setRegression('ridge').begin().showPredictionPoints(true);
            calibrateWebGazer();
        };

        // Calibration function
        function calibrateWebGazer() {
            let calibrationPoints = [[50, 50], [window.innerWidth - 50, 50], [window.innerWidth - 50, window.innerHeight - 50], [50, window.innerHeight - 50]];
            calibrationPoints.forEach(point => {
                let pointElem = document.createElement('div');
                pointElem.style.position = "absolute";
                pointElem.style.left = point[0] + 'px';
                pointElem.style.top = point[1] + 'px';
                pointElem.style.width = "20px";
                pointElem.style.height = "20px";
                pointElem.style.backgroundColor = "red";
                document.body.appendChild(pointElem);

                pointElem.addEventListener('click', () => {
                    webgazer.recordScreenPosition(point[0], point[1]);
                });
            });
        }

        // Define JsPsych timeline
        let experiment_timeline = [];

        let trial = {
            type: 'html-keyboard-response',
            stimulus: '<p>Look at the center of the screen</p>',
            on_load: function() {
                webgazer.setGazeListener(function(data, timestamp) {
                    if (data) {
                        jsPsych.data.write({
                            x: data.x,
                            y: data.y,
                            time: timestamp
                        });
                    }
                });
            },
            on_finish: function() {
                webgazer.clearGazeListener();
            }
        };

        experiment_timeline.push(trial);

        jsPsych.init({
            timeline: experiment_timeline,
            on_finish: function() {
                jsPsych.data.displayData();
            }
        });
    </script>
</body>
</html>



